"""Native compilation for maximum performance.

Compiles IR to native Rust binary. Auto-installs Rust toolchain if needed.

Performance: ~40M rows/sec with numpy arrays.
"""

import hashlib
import json
import os
import platform
import shutil
import struct
import subprocess
import tempfile
from pathlib import Path

import numpy as np

from .compiler import IR
from .codegen.rust import generate_rust

CACHE_DIR = Path.home() / ".cache" / "rac"
RUSTUP_URL = "https://sh.rustup.rs"


def _get_cargo() -> Path | None:
    """Find cargo binary."""
    cargo = shutil.which("cargo")
    if cargo:
        return Path(cargo)

    cargo_home = Path.home() / ".cargo" / "bin" / "cargo"
    if cargo_home.exists():
        return cargo_home

    return None


def _install_rust() -> Path:
    """Install Rust via rustup."""
    print("Installing Rust toolchain (one-time setup)...")

    if platform.system() == "Windows":
        import urllib.request
        rustup_init = CACHE_DIR / "rustup-init.exe"
        CACHE_DIR.mkdir(parents=True, exist_ok=True)
        urllib.request.urlretrieve("https://win.rustup.rs/x86_64", rustup_init)
        subprocess.run([str(rustup_init), "-y", "--quiet"], check=True)
    else:
        subprocess.run(
            ["sh", "-c", f"curl --proto '=https' --tlsv1.2 -sSf {RUSTUP_URL} | sh -s -- -y --quiet"],
            check=True,
            capture_output=True
        )

    cargo = Path.home() / ".cargo" / "bin" / "cargo"
    if not cargo.exists():
        raise RuntimeError("Failed to install Rust")

    print("Rust installed successfully")
    return cargo


def ensure_cargo() -> Path:
    """Ensure cargo is available, installing if needed."""
    cargo = _get_cargo()
    if cargo:
        return cargo
    return _install_rust()


def _ir_hash(ir: IR) -> str:
    """Hash IR for caching."""
    data = json.dumps({
        "order": ir.order,
        "vars": {k: str(v.expr) for k, v in ir.variables.items()}
    }, sort_keys=True)
    return hashlib.sha256(data.encode()).hexdigest()[:16]


class CompiledBinary:
    """A compiled RAC binary for maximum performance.

    Handles multiple entity tables with foreign key relationships.
    Each entity is processed in parallel; cross-entity lookups use indexes.
    """

    def __init__(self, binary_path: Path, ir: IR, entity_schemas: dict[str, list[str]]):
        """
        Args:
            binary_path: Path to compiled binary
            ir: The IR this was compiled from
            entity_schemas: Dict of entity_name -> list of input field names
        """
        self.binary_path = binary_path
        self.ir = ir
        self.entity_schemas = entity_schemas

        # Build output fields per entity
        self.entity_outputs: dict[str, list[str]] = {}
        for path in ir.order:
            var = ir.variables[path]
            if var.entity:
                if var.entity not in self.entity_outputs:
                    self.entity_outputs[var.entity] = []
                self.entity_outputs[var.entity].append(path)

    def run(self, data: dict[str, list[dict]] | dict[str, np.ndarray]) -> dict[str, np.ndarray]:
        """Run the binary on relational data.

        Args:
            data: Dict of entity_name -> rows (list of dicts or numpy array)
                  Each entity table should have an 'id' field for foreign keys.

        Returns:
            Dict of entity_name -> numpy array of computed values
            Array columns correspond to output variables in IR order.

        Example:
            >>> data = {
            ...     'household': [{'id': 1, 'region': 'london'}],
            ...     'person': [{'id': 1, 'household_id': 1, 'income': 50000}],
            ... }
            >>> result = binary.run(data)
            >>> result['person']  # shape (n_persons, n_output_vars)
        """
        results = {}

        for entity_name, rows in data.items():
            if entity_name not in self.entity_outputs:
                continue  # No variables to compute for this entity

            input_fields = self.entity_schemas.get(entity_name, [])
            output_fields = self.entity_outputs[entity_name]

            # Convert to numpy if needed
            if isinstance(rows, np.ndarray):
                input_arr = rows.astype(np.float64, copy=False)
                n_rows = len(input_arr)
            else:
                n_rows = len(rows)
                if n_rows == 0:
                    results[entity_name] = np.zeros((0, len(output_fields)), dtype=np.float64)
                    continue

                input_arr = np.array([
                    [float(row.get(field, 0.0)) for field in input_fields]
                    for row in rows
                ], dtype=np.float64)

            # Write binary input
            input_path = tempfile.mktemp(suffix='.bin')
            with open(input_path, 'wb') as f:
                f.write(struct.pack('<Q', n_rows))
                input_arr.tofile(f)

            output_path = tempfile.mktemp(suffix='.bin')

            try:
                result = subprocess.run(
                    [str(self.binary_path), entity_name, input_path, output_path],
                    capture_output=True,
                    text=True
                )
                if result.returncode != 0:
                    raise RuntimeError(f"Binary failed for {entity_name}: {result.stderr}")

                # Read binary output
                with open(output_path, 'rb') as f:
                    out_n = struct.unpack('<Q', f.read(8))[0]
                    output_arr = np.fromfile(f, dtype=np.float64).reshape(out_n, len(output_fields))

                results[entity_name] = output_arr

            finally:
                os.unlink(input_path)
                if os.path.exists(output_path):
                    os.unlink(output_path)

        return results


def compile_to_binary(ir: IR, cache: bool = True) -> CompiledBinary:
    """Compile IR to native binary.

    Args:
        ir: Compiled IR from rac.compile()
        cache: Cache compiled binaries (default True)

    Returns:
        CompiledBinary that processes ~40M rows/sec with numpy input

    Example:
        >>> ir = compile([module], as_of=date(2024, 6, 1))
        >>> binary = compile_to_binary(ir)
        >>> result = binary.run({
        ...     'person': np.array([[50000], [75000], [100000]])  # income column
        ... })
    """
    cargo = ensure_cargo()

    # Get entity schemas from IR
    entity_schemas: dict[str, list[str]] = {}
    entity_outputs: dict[str, list[str]] = {}

    for path in ir.order:
        var = ir.variables[path]
        if var.entity:
            if var.entity not in entity_outputs:
                entity_outputs[var.entity] = []
            entity_outputs[var.entity].append(path)

    for entity_name in entity_outputs:
        if entity_name in ir.schema_.entities:
            entity_schemas[entity_name] = list(ir.schema_.entities[entity_name].fields.keys())
        else:
            entity_schemas[entity_name] = []

    ir_hash = _ir_hash(ir)
    project_dir = CACHE_DIR / "projects" / ir_hash

    binary_name = "rac_native.exe" if platform.system() == "Windows" else "rac_native"
    binary_path = project_dir / "target" / "release" / binary_name

    if cache and binary_path.exists():
        return CompiledBinary(binary_path, ir, entity_schemas)

    # Create Cargo project
    project_dir.mkdir(parents=True, exist_ok=True)

    (project_dir / "Cargo.toml").write_text('''[package]
name = "rac_native"
version = "0.1.0"
edition = "2021"

[dependencies]
rayon = "1.10"

[profile.release]
lto = true
codegen-units = 1
''')

    # Generate Rust code
    rust_code = generate_rust(ir)
    main_code = _generate_main(ir, entity_schemas, entity_outputs)

    # Combine with lint suppression at top
    full_code = "#![allow(unused_parens)]\n\n" + rust_code + "\n" + main_code

    src_dir = project_dir / "src"
    src_dir.mkdir(exist_ok=True)
    (src_dir / "main.rs").write_text(full_code)

    # Build
    print("Compiling native binary...")
    result = subprocess.run(
        [str(cargo), "build", "--release", "--quiet"],
        cwd=project_dir,
        capture_output=True,
        text=True
    )

    if result.returncode != 0:
        raise RuntimeError(f"Compilation failed:\n{result.stderr}")

    print("Compilation complete")
    return CompiledBinary(binary_path, ir, entity_schemas)


def _generate_main(
    ir: IR,
    entity_schemas: dict[str, list[str]],
    entity_outputs: dict[str, list[str]]
) -> str:
    """Generate main function that handles multiple entities."""

    entity_handlers = []

    for entity_name, input_fields in entity_schemas.items():
        output_fields = entity_outputs.get(entity_name, [])
        if not output_fields:
            continue

        type_name = "".join(part.capitalize() for part in entity_name.split("_"))
        n_inputs = len(input_fields)
        n_outputs = len(output_fields)

        # Field reads (with trailing commas and type casts)
        field_reads = []
        for i, f in enumerate(input_fields):
            # Check field type from schema
            entity_schema = ir.schema_.entities.get(entity_name)
            if entity_schema and f in entity_schema.fields:
                dtype = entity_schema.fields[f].dtype
                if dtype == "int":
                    field_reads.append(f"                    {f}: row[{i}] as i64,")
                else:
                    field_reads.append(f"                    {f}: row[{i}],")
            else:
                field_reads.append(f"                    {f}: row[{i}],")

        # Output writes
        output_writes = [
            f"            out[{i}] = o.{path.replace('/', '_')};"
            for i, path in enumerate(output_fields)
        ]

        entity_handlers.append(f'''
        "{entity_name}" => {{
            let n_input_fields = {n_inputs};
            let n_output_fields = {n_outputs};

            let mut input_data = vec![0.0f64; n_rows * n_input_fields];
            for i in 0..n_rows * n_input_fields {{
                file.read_exact(&mut buf8).expect("Failed to read");
                input_data[i] = f64::from_le_bytes(buf8);
            }}

            let mut output_data = vec![0.0f64; n_rows * n_output_fields];

            input_data
                .par_chunks(n_input_fields)
                .zip(output_data.par_chunks_mut(n_output_fields))
                .for_each(|(row, out)| {{
                    let input = {type_name}Input {{
{chr(10).join(field_reads)}
                    }};
                    let o = {type_name}Output::compute(&input, &scalars);
{chr(10).join(output_writes)}
                }});

            // Write output
            out_file.write_all(&(n_rows as u64).to_le_bytes()).unwrap();
            for v in output_data {{
                out_file.write_all(&v.to_le_bytes()).unwrap();
            }}
        }}''')

    return f'''
use rayon::prelude::*;
use std::env;
use std::fs::File;
use std::io::{{Read, Write, BufReader, BufWriter}};

fn main() {{
    let args: Vec<String> = env::args().collect();
    if args.len() != 4 {{
        eprintln!("Usage: {{}} <entity> <input.bin> <output.bin>", args[0]);
        std::process::exit(1);
    }}

    let entity = &args[1];
    let mut file = BufReader::new(File::open(&args[2]).expect("Failed to open input"));
    let mut out_file = BufWriter::new(File::create(&args[3]).expect("Failed to create output"));

    let mut buf8 = [0u8; 8];
    file.read_exact(&mut buf8).expect("Failed to read count");
    let n_rows = u64::from_le_bytes(buf8) as usize;

    let scalars = Scalars::compute();

    match entity.as_str() {{
{chr(10).join(entity_handlers)}
        _ => {{
            eprintln!("Unknown entity: {{}}", entity);
            std::process::exit(1);
        }}
    }}
}}
'''
